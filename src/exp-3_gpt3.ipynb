{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEYS = \"\"\"keys_here\"\"\"\n",
    "key_list = API_KEYS.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "from openai.error import RateLimitError\n",
    "import argparse\n",
    "seed = 34\n",
    "key_id = 0\n",
    "pairs_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'text-davinci-002'\n",
    "time_stamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n",
    "random.seed(seed)\n",
    "os.environ['OPENAI_API_KEY'] = key_list[key_id]\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../dataset/test.csv\", encoding=\"utf-8\")\n",
    "df_train = pd.read_csv('../dataset/train.csv', encoding='utf-8')\n",
    "test_dataset = df_test[\"question\"]\n",
    "label_dataset = df_test[\"label\"]\n",
    "train_dataset = df_train['question']\n",
    "answer_dataset = df_train['answer']\n",
    "test_list = []\n",
    "label_list = []\n",
    "for question, label in zip(test_dataset, label_dataset):\n",
    "    test_list.append(question)\n",
    "    label_list.append(label)\n",
    "\n",
    "\n",
    "prompt = \"\"\"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will say \"tricky question.\" first and give the reason, otherwise I will say \"true question.\" first and give the reason.\"\"\"\n",
    "prompt_list = []\n",
    "sample_string = ''\n",
    "pos_ids = random.sample(range(int(len(train_dataset) / 2)), pairs_num)\n",
    "neg_ids = [num + int(len(train_dataset) / 2) for num in pos_ids]\n",
    "pos_questions = [train_dataset[id] for id in pos_ids]\n",
    "neg_questions = [train_dataset[id] for id in neg_ids]\n",
    "pos_answers = [answer_dataset[id] for id in pos_ids]\n",
    "neg_answers = [answer_dataset[id] for id in neg_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos_q, neg_q, pos_a, neg_a in zip(pos_questions, neg_questions, pos_answers, neg_answers):\n",
    "    sample_string += f'\\n\\nQ: {pos_q}\\nA:tricky question. {pos_a}\\n\\nQ: {neg_q}\\nA:true question. {neg_a}'\n",
    "for question in test_list:\n",
    "    processed_string = f\"{prompt}{sample_string}\\n\\nQ: {question}\\nA:\"\n",
    "    prompt_list.append(processed_string)\n",
    "print(\"length of prompt_list: \" + str(len(prompt_list)))\n",
    "print()\n",
    "print('prompt and sample string: ')\n",
    "print(f'{prompt}{sample_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "response_list = []\n",
    "count = 0\n",
    "error_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_date_key(key_id):\n",
    "    key_id += 1\n",
    "    os.environ['OPENAI_API_KEY'] = key_list[key_id]\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    return key_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_error = 0\n",
    "error_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prompt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompt_list[count:]:\n",
    "    response = openai.Completion.create(\n",
    "    engine=model_name,\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=32,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    stop=None\n",
    "    )\n",
    "    response['question'] = test_list[count]\n",
    "    response['label'] = label_list[count]\n",
    "    response_list.append(response)\n",
    "    count += 1\n",
    "    time.sleep(2)\n",
    "    if count % 200 == 0:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_id = up_date_key(key_id)\n",
    "print(key_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count)\n",
    "print(len(response_list))\n",
    "print(response_list[count - 1])\n",
    "print(test_list[count - 1])\n",
    "print(key_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../output/exp-3/incontext/exp-3_gpt3_scale-{pairs_num}_{model_name}_{time_stamp}_{str(seed)}.json\", \"a\") as f:\n",
    "    json.dump(response_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../output/exp-3/incontext/exp-3_gpt3_scale-{pairs_num}_{model_name}_{time_stamp}_{str(seed)}.json\", \"r\") as f:\n",
    "    output_dict = {'question': [], 'answer': [], 'label': []}\n",
    "    data = json.load(f)\n",
    "    for response in data:\n",
    "        label = response['label']\n",
    "        question = response['question']\n",
    "        answer = response['choices'][0]['text']\n",
    "        output_dict['label'].append(label)\n",
    "        output_dict['question'].append(question)\n",
    "        output_dict['answer'].append(answer.replace(\"\\n\", ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output_dict)\n",
    "df.to_csv(f\"../output/exp-3/incontext/exp-3_gpt3_scale-{pairs_num}_{model_name}_{time_stamp}_{str(seed)}.csv\")\n",
    "print('result saved in ' + f\"../output/exp-3/incontext/exp-3_gpt3_scale-{pairs_num}_{model_name}_{time_stamp}_{str(seed)}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:16:26) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0d97bcb8eb2c5c8bf713bbf333656d5333558e3a67524bffa742df36669829f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
